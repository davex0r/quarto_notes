[
  {
    "objectID": "tech_posts/sloppy_projection/sloppy_projection.html",
    "href": "tech_posts/sloppy_projection/sloppy_projection.html",
    "title": "An interesting connection between sloppy model analysis and projection operators",
    "section": "",
    "text": "This note concerns various ways I have been thinking about basis functions with connections between some other fields I have been interested in, namely, projection operators, Koopman and Transfer operators, coordinate transformations, “sloppy model analysis”, Hilbert spaces and Reproducing Kernel Hilbert spaces, and function approximation.\nFunction approximation is a useful tool, it is no coincidence that artificial neural networks of the multilayer feedforward variety 1 2 are provably universal function approximators. Projection in a Hilbert space of functions is one method of function approximation. A familiar example of a Hilbert space projection methods is Fourier analysis, where an arbitrary function is represented by its projection onto a basis set of functions, in this case sinusoids. Function projection relies on having an inner product on the function space, and in this case we will use the \\(L^2\\) inner product defined as, \\[ \\langle f(t),g(t)\\rangle = \\int_a^b(f(t) \\cdot g(t)) dt\\] With an increasing number of terms in the Fourier series, we can approximate a given function to arbitrary accuracy. Truncating the series is a form of projection, where we are projecting the infinite dimensional vector that represents the function onto the subspace spanned by only a finite set of modes, for example onto the lower frequency modes. The projection need not be a frequency cutoff, one could choose arbitrarily some subspace on which to project the function, for example, a custom compression for that function might choose the n modes with the highest power. This is an example of what I call “Concentration of Dimension”3 and may provide a basis for understanding the emergence of low-dimensionality in biological systems and in particular how these systems are capable of both canalization and plasticity.\nIn general we can represent an arbitrary function in any basis by projecting it onto the span of the subspace of those basis functions. It is easiest if those functions comprise an orthonormal basis, as they do in the Fourier series example, but this is not necessary, in fact we don’t even have to orthogonalize the basis first if we can compute and invert the Gram Matrix (the matrix of inner products between the basis functions). This is the basic idea behind regularization in function approximation and techniques such as kernel regression. This should sound eerily familiar re: Orthogonality!\nAt this point I would like to present a simple example which will also highlight the connection to sloppy model analysis. Taylor series approximation is a well known example of function approximation in a polynomial basis, usually motivated as a local equivalence between the n derivatives of the function and those of the polynomial approximation. However we can also view polynomial approximation as a projection onto polynomial basis functions. For example, the second order Taylor approximation of a function \\(f(x)\\) can be viewed as the projection of the function \\(f(x)\\) onto the subspace spanned by \\(1\\), \\(x\\), and \\(x^2\\). In a procedure similar to what we did wth orthogonality, we can first compute the projection onto the basis functions even though the basis functions are not orthonormal. For this example lets use \\(f(x) = sin(x)\\) and the interval \\([a,b] = [0,1]\\) to match the conditions in Sethna’s work. As a reminder, we are going to use function space projection to find the coefficients \\(w_i\\) in the nth order polynomial approximation \\(\\sum_{i=0}^n(w_it^i)\\) . For the polynomial approximation, this can be written as minimization problem \\[min_{w\\in R^n}\\left\\lVert f-\\sum_{i=0}^n(w_it^i) \\right\\rVert\\] noting that \\[ \\left\\lVert f-\\sum_{i=0}^n(w_it^i) \\right\\rVert^2 = \\langle f-\\sum_{i=0}^nw_it^i,f-\\sum_{i=0}^nw_it^i \\rangle\\]\nand using the inner product defined above, we can derive the Projection in terms of the Gram Matrix with the projection is given by \\[\\begin{align}w^* &= G^{-1} \\left [ \\begin{matrix} \\langle f,b_0\\rangle \\\\  \\langle f,b_1\\rangle \\\\ \\langle f,b_2\\rangle \\\\ ... \\\\ \\langle f,b_n\\rangle \\\\ \\end{matrix} \\right]\\end{align}\\]\nwith the Gram matrix given as the \\(nxn\\) matrix of inner products between the basis functions. If the basis functions form an orthonormal basis, then the Gram matrix is equal to the identity matrix, and it is equal to its own inverse. However, this need to be true and in general the Gram matrix is given as \\[ G =  \\left [ \\begin{matrix} \\langle b_0,b_0\\rangle & \\langle b_1,b_0\\rangle & ... & \\langle b_n,b_0\\rangle \\\\  \\langle b_0,b_1\\rangle & \\langle b_1,b_1\\rangle & ... & \\langle b_n,b_1\\rangle  \\\\ ... & ... & ... & ...  \\\\ \\langle b_0,b_n\\rangle & \\langle b_1,b_n\\rangle & ... & \\langle b_n,b_n\\rangle  \\\\ \\end{matrix} \\right]\\]\nWith our definition of the inner product and the monomial basis functions, we can compute this gram Matrix explicitly for polynomial projection.\n\\[\\begin{align} \\langle b_0,b_0\\rangle &= \\int_0^1(1\\cdot 1)dt = x|_0^1 = 1 \\\\ \\langle b_0,b_1\\rangle = \\langle b_1,b_0\\rangle &= \\int_0^1(1\\cdot x)dt = \\frac{x^2}{2}|_0^1 = \\frac{1}{2} \\\\ \\langle b_1,b_1\\rangle &= \\int_0^1(x\\cdot x)dt = \\frac{x^3}{3}|_0^1 = \\frac{1}{3} \\\\ \\langle b_0,b_2\\rangle = \\langle b_2,b_0\\rangle &= \\int_0^1(1\\cdot x^2)dt = \\frac{x^3}{3}|_0^1 = \\frac{1}{3} \\\\\n\\langle b_1,b_2\\rangle = \\langle b_2,b_1\\rangle &= \\int_0^1(x\\cdot x^2)dt = \\frac{x^4}{4}|_0^1 = \\frac{1}{4} \\\\\n\\langle b_2,b_2\\rangle &= \\int_0^1(x^2\\cdot x^2)dt = \\frac{x^5}{5}|_0^1 = \\frac{1}{5} \\\\\n\\end{align}\\]\nSo in this case, the final weights are given by\n\\[ \\begin{align}\nw^* &= G^{-1} \\left [ \\begin{matrix} \\langle sin(x),1\\rangle \\\\  \\langle sin(x),x\\rangle \\\\ \\langle sin(x),x^2\\rangle \\\\ \\end{matrix} \\right] \\\\\n&= \\left [ \\begin{matrix} 1 & 1/2 & 1/3 \\\\ 1/2 & 1/3 & 1/4 \\\\ 1/3 & 1/4 & 1/5 \\end{matrix} \\right ]^{-1}*\\left [ \\begin{matrix} 0.4597 \\\\ 0.3012 \\\\ 0.2232 \\end{matrix} \\right ]\n\\end{align}\\]\nWhich gives the following results:\n::: {#cell-non-linear fit .cell execution_count=1}\nPolynomial fits using either Taylor series approximation or the optimal projection :::\nNow let us look at the the general Gram matrix in this case, we obtain the Hilbert matrix. The fact that this matrix is ill conditioned means that the inverse greatly magnifies small differences in the input. \\[\\begin{align}G &= H_{ij}=\\frac{1}{(i+j+1)} \\\\ &= \\left [ \\begin{matrix} 1 & \\frac{1}{2} & \\frac{1}{3} & ... \\\\  \\frac{1}{2} & \\frac{1}{3} & ...& ... \\\\ \\frac{1}{3} & ... & ... & ... \\\\ ... & ... & ... & ...  \\end{matrix} \\right]\\end{align}\\] I was struck when this matrix appeared because I had seen it before in the Sloppy model literature4 but derived in a very different context. In sloppy model analysis, we are interested in the looking at the parameter sensitivity of a continuous least squares regression. This sensitivity is captured by the Hessian of the fit function with respect to the parameters at the best fit point, so in this case, we are looking at the matrix given by\n\\[ \\frac{\\partial^2}{\\partial w_i \\partial w_j}\\left (\\frac{1}{2}\\int_0^1\\sum_n(w_it^i-w_i^*t^i)^2 dt \\right )\\]\nSurprising to me, this gives the exact same matrix as the Gram matrix for the projection operator.\n\\[\\begin{align}H_{n,m} &= \\frac{\\partial^2}{\\partial w_n \\partial w_m}\\left (\\frac{1}{2}\\int_0^1\\sum_n(w_it^i-w_i^*t^i)^2 dt\\right ) \\\\\n&= \\frac{1}{2}\\int_0^1 \\frac{\\partial^2}{\\partial w_n \\partial w_m} \\sum_n(w_it^i-w_i^*t^i)^2 dt \\\\\n&= \\frac{1}{2}\\int_0^1 \\frac{\\partial^2}{\\partial w_n \\partial w_m} \\left ( \\sum_n(w_it^i)^2-2\\sum_n(w_it^i*w_i^*t^i)+\\sum_n(w_i^*t^i)^2 \\right )dt \\\\\n&= \\frac{1}{2}\\int_0^1 \\frac{\\partial}{\\partial w_n} \\left ( 2\\sum_n(w_it^i)*t^m-2(t^m*w_m^*t^m) \\right )dt \\\\\n&= \\frac{1}{2}\\int_0^1 \\left ( 2t^n*t^m \\right )dt \\\\\n&= \\int_0^1 t^{(n+m)} dt \\\\\n&= \\frac{1}{n+m+1}t^{n+m+1}\\Big|_0^1 \\\\\n&= \\frac{1}{n+m+1}\n\\end{align}\\]\nThis leads me to believe that I am on the right track thinking about my worm developmental biology project, my worm behavior project, and our non equilibrium stuff in terms of projection operator theory (a convergence which emerged very unexpectedly at three different scales of inquiry)"
  },
  {
    "objectID": "tech_posts/sloppy_projection/sloppy_projection.html#footnotes",
    "href": "tech_posts/sloppy_projection/sloppy_projection.html#footnotes",
    "title": "An interesting connection between sloppy model analysis and projection operators",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHartman EJ, Keeler JD & Kowalski JM (1990) Layered neural networks with Gaussian hidden units as universal approximations. Neural Comput 2: 210–215↩︎\nHornik K, Stinchcombe M & White H (1989) Multilayer feedforward networks are universal approximators. Neural Netw 2: 359–366↩︎\nJordan DJ & Miska EA (2023) Canalisation and plasticity on the developmental manifold of Caenorhabditis elegans. Mol Syst Biol: e11835↩︎\nWaterfall JJ, Casey FP, Gutenkunst RN, Brown KS, Myers CR, Brouwer PW, Elser V & Sethna JP (2006) Sloppy-model universality class and the Vandermonde matrix. Phys Rev Lett 97: 150601↩︎"
  },
  {
    "objectID": "tech_posts/observable_chem/observable__chem.html",
    "href": "tech_posts/observable_chem/observable__chem.html",
    "title": "Projections and Basis Functions with Chemistry",
    "section": "",
    "text": "This is a short note which connects how well chosen changes to the chemical kinetics in a simple system can manifest mathematically as either altering the projections onto a constant set of basis functions, or altering the basis functions while maintaining the projection. This note is intended to provide a very basic foundation for a more sophisticated model of learning and evolution in physical/chemical systems.\nGiven a simple chemical reaction\n\\[\n\\Large\n\\ce{a&lt;=&gt;[{\\lambda}][{\\nu}]b}\n\\]\nRecall from the previous note the simple conversion reaction with dynamics given by: \\[\\frac{d}{dt}\\begin{bmatrix}a\\\\b\\end{bmatrix} = \\begin{bmatrix}-\\lambda & \\nu \\\\ \\lambda & -\\nu\\end{bmatrix}\\begin{bmatrix}a\\\\b\\end{bmatrix}\\]\nSome of the formulas I presented without derivation in the previous notes were derived from looking at the eigen-decomposition of the Laplacian. \\[\\large  L = UDU^{-1} \\] This gives us the eigenvector matrix \\[ U = \\begin{bmatrix} \\frac{\\nu}{\\lambda} & -1 \\\\ 1 & 1 \\end{bmatrix}, D = \\begin{bmatrix} 0 & 0 \\\\ 0 & -(\\nu+\\lambda) \\end{bmatrix} \\] We can use this transformation to define new set of uncoupled coordinates \\[\\large \\begin{align}\\dot{\\vec{x}} = L\\vec{x} &= UDU^{-1}\\vec{x} \\\\\\dot{\\vec{x}}&= UDU^{-1}\\vec{x} \\\\  U^{-1}\\dot{\\vec{x}}&=  DU^{-1}\\vec{x}\\end{align}\\]If we define \\(z=U^{-1}x\\) then we now have a dynamical system of 2 variables that do not interact (compare to the original \\(a,b\\) sytem which has interactions) \\[\\large \\frac{d\\vec{z}}{dt} = Dz = \\begin{bmatrix} 0 & 0 \\\\ 0 & -(\\nu+\\lambda) \\end{bmatrix}z\\] Thus: \\[\n\\begin{align} dz_1/dt &= 0 \\\\ dz_2/dt &= -(\\nu+\\lambda)z_2 \\end{align}\n\\] which gives \\[ \\Large\n\\begin{align} z_1(t) &= z_1(0) \\\\ z_2(t) &= z_2(0)e^{-(\\nu+\\lambda)t}\\end{align}\n\\] Noting that \\(z_1 = \\frac{\\lambda}{\\lambda+\\nu}a+\\frac{\\lambda}{\\lambda+\\nu}b\\) and \\(z_2 = \\frac{-\\lambda}{\\lambda+\\nu}a+\\frac{\\nu}{\\lambda+\\nu}b\\), which is how I derived the observable function \\(c\\) in the note [[Introduction and Motivations for a Projection Operator Approach]]. \\(z_1\\) represents the total (probability) mass and \\(\\dot{z_1}=0\\) reflects conservation of (probability) mass.\nWe’ve carried out this eigen-decomposition because the natural dynamics of \\(a\\) ad \\(b\\) can be represented as a linear combination of the uncoupled dynamics of \\(z_1\\) and \\(z_2\\). In the language I have been developing in the last few notes, I would say that \\(z_1(t)\\) and \\(z_2(t)\\) are the basis functions and the dynamics of \\(a\\) and \\(b\\) are projections onto these basis functions.\nIt is clear in this simple example that the basis functions are defined by the eigenvalues and the projection is defined by the eigenvectors. Thus if we change the eigenvalues without changing the eigenvectors we adjust the basis functions ad if we change the eigenvectors while keeping constant eigenvalues we change the projection.\nLets now consider the action of an enzyme which lowers the activation energy barrier for the reaction. This results in a proportional increase in both \\(\\nu\\) and \\(\\lambda\\) \\[\n\\large\n\\ce{a&lt;=&gt;[{r\\lambda}][{r\\nu}]b}\n\\] The eigenvector associated with the 0 eigenvalue is then \\(\\frac{r\\nu}{r\\lambda} = \\frac{\\nu}{\\lambda}\\), and is thus unchanged. However, the non-zero eigenvalue is now \\(-(r\\nu+r\\lambda)=-r(\\nu+\\lambda)\\). This results in the second basis function changing to \\[ \\large z_2(t) = z_2(0)e^{-r(\\nu+\\lambda)t}\\]Because the dynamics of \\(a\\) and \\(b\\) are linear combinations of \\(z_1\\) and \\(z_2\\) and the dynamics of \\(z_2\\) are now much faster (by a factor of \\(r\\)) the resulting dynamics of a and b will be faster, but the resulting steady state concentrations of \\(a\\) and \\(b\\) will be unchanged. This matches our intuition based on activation energy changes in equilibrium systems.\nAlternatively, in this simple system, we can adjust the projections without changing the basis functions. We do this by adjusting \\(\\nu\\) and \\(\\lambda\\) such that their sum stays constant. This corresponds to a process which maintains the total flux in the system but adjusts the flux balance1 (this necessarily corresponds to binding energy differences here). \\[\\begin{align} \\lambda \\to (\\lambda + r) \\\\ \\nu\\to(\\nu-r)\\end{align}\\]This changes the steady state ratio of \\(a\\) and \\(b\\) but leaves the kinetics unchanged. This is because the basis functions are unchanged: \\[ \\large z_2(t) = z_2(0)e^{-(\\nu+r+\\lambda-r)t} = z_2(0)e^{-(\\nu+\\lambda)t}\\]This uncoupling of the dynamics into independent observable functions \\(z\\) is always possible for Laplacian dynamics. To connect it to some of the concepts introduced in previous notes, we can think of the functions \\(z\\) as observable functions of the natural coordinates \\(a\\) and \\(b\\), where each observable function has dynamics that are closed on its own 1D subspace (this is a restatement of the uncoupling condition). We can also think of the dynamics of \\(a\\) and \\(b\\) as being projections onto the basis functions defined by \\(z\\). Interestingly the basis functions \\(z\\) define a spectrum of time-scales for the dynamics because the dynamics of the molecular concentrations will be linear combinations of the basis functions defined by the eigenvalue spectrum. In the next note I will explore how these notions can be applied to larger networks, and look at some connections to kinetic regime vs energetic regime proofreading, building finally to connections to orthogonality."
  },
  {
    "objectID": "tech_posts/observable_chem/observable__chem.html#footnotes",
    "href": "tech_posts/observable_chem/observable__chem.html#footnotes",
    "title": "Projections and Basis Functions with Chemistry",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis possibly implies the existence of a basic chemical reaction modification (like an enzyme) that shifts the flux balance of a reaction while preserving the total reaction flux. I could not think of a ready biological example of such a thing, but I would keep an eye out for something that fits this.↩︎"
  },
  {
    "objectID": "tech_posts/gram_projection/gram_projection.html",
    "href": "tech_posts/gram_projection/gram_projection.html",
    "title": "Projection in terms of the Gram Matrix",
    "section": "",
    "text": "This is a quick note showing how the optimal projection can be derived in terms of the inversion of the gram matrix used in the note on the connection between sloppiness and projection.\nLets derive \\(min_{w\\in R^n}\\left\\lVert f-\\sum_{i=0}^nw_it^i) \\right\\rVert^2\\) with projections (this is the best nth degree polynomial approximation of a function f)\n\\[min_{w\\in R^n}\\left\\lVert f-\\sum_{i=0}^nw_it^i) \\right\\rVert^2 = min_{w\\in R^n}\\left\\langle f-\\sum_{i=0}^nw_it^i,f-\\sum_{i=0}^nw_it^i \\right\\rangle\\]\nby definition of the norm in terms of the inner product. \\[ \\begin{align} \\left\\langle \\cdot,\\cdot \\right\\rangle &=  \\int_0^1(f-\\sum_{i=0}^nw_it^i)^2 dt \\\\ &= \\int_0^1(f^2-2f\\left(\\sum_{i=0}^nw_it^i\\right) + \\left(\\sum_{i=0}^nw_it^i\\right)^2 dt \\\\ &= \\int_0^1(f^2) -\\int_0^1 2f\\left(\\sum_{i=0}^nw_it^i\\right) + \\int_0^1\\left(\\sum_{i=0}^nw_it^i\\right)^2 dt \\\\ \\end{align}\\] \\[F(w) = \\left\\langle f \\right\\rangle^2-2w^\\intercal \\begin{bmatrix} \\left\\langle f,1 \\right\\rangle \\\\ \\left\\langle f,t\\right\\rangle\\\\ \\left\\langle f,t \\right\\rangle \\\\ ... \\\\ \\left\\langle f,t^n \\right\\rangle\\end{bmatrix} +w^\\intercal \\begin{bmatrix} \\left\\langle 1,1 \\right\\rangle & \\left\\langle t,1 \\right\\rangle & ... & \\left\\langle t^n,1 \\right\\rangle \\\\ \\left\\langle 1,t\\right\\rangle\\\\ \\left\\langle 1,t^2 \\right\\rangle \\\\ ... \\\\ \\left\\langle 1,t^n \\right\\rangle  & ... &  &\\left\\langle t^n,t^n \\right\\rangle \\end{bmatrix}w\\] remembering that we are minimizing \\(F\\) with respect to \\(w\\). We can find the minimum by solving \\(\\nabla_w F=0\\), the first term is zero, lets expand the second term \\[\\begin{align} \\nabla_w\\left( -2w^\\intercal \\begin{bmatrix} \\left\\langle f,1 \\right\\rangle \\\\ \\left\\langle f,t\\right\\rangle\\\\ \\left\\langle f,t \\right\\rangle \\\\ ... \\\\ \\left\\langle f,t^n \\right\\rangle\\end{bmatrix} \\right) &= -2\\begin{bmatrix} \\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1} \\\\\\frac{\\partial}{\\partial w_2}\\\\... \\end{bmatrix} \\left( w_0\\left\\langle f,1 \\right\\rangle +w_1\\left\\langle f,t \\right\\rangle + w_2\\left\\langle f,t^2 \\right\\rangle + ... \\right) \\\\ &= -2\\begin{bmatrix} \\left\\langle f,1 \\right\\rangle \\\\ \\left\\langle f,t\\right\\rangle\\\\ \\left\\langle f,t \\right\\rangle \\\\ ... \\\\ \\left\\langle f,t^n \\right\\rangle\\end{bmatrix} \\end{align}\\]\nIt is a bit more complicated, but \\[\\nabla_w\\left(w^\\intercal \\begin{bmatrix} \\left\\langle 1,1 \\right\\rangle & \\left\\langle t,1 \\right\\rangle & ... & \\left\\langle t^n,1 \\right\\rangle \\\\ \\left\\langle 1,t\\right\\rangle\\\\ \\left\\langle 1,t^2 \\right\\rangle \\\\ ... \\\\ \\left\\langle 1,t^n \\right\\rangle  & ... &  &\\left\\langle t^n,t^n \\right\\rangle \\end{bmatrix}w \\right)= 2\\begin{bmatrix} \\left\\langle 1,1 \\right\\rangle & \\left\\langle t,1 \\right\\rangle & ... & \\left\\langle t^n,1 \\right\\rangle \\\\ \\left\\langle 1,t\\right\\rangle\\\\ \\left\\langle 1,t^2 \\right\\rangle \\\\ ... \\\\ \\left\\langle 1,t^n \\right\\rangle  & ... &  &\\left\\langle t^n,t^n \\right\\rangle \\end{bmatrix}w\\] So we have \\[ 0 = -2\\begin{bmatrix} \\left\\langle f,1 \\right\\rangle \\\\ \\left\\langle f,t\\right\\rangle\\\\ \\left\\langle f,t \\right\\rangle \\\\ ... \\\\ \\left\\langle f,t^n \\right\\rangle\\end{bmatrix} + 2\\begin{bmatrix} \\left\\langle 1,1 \\right\\rangle & \\left\\langle t,1 \\right\\rangle & ... & \\left\\langle t^n,1 \\right\\rangle \\\\ \\left\\langle 1,t\\right\\rangle\\\\ \\left\\langle 1,t^2 \\right\\rangle \\\\ ... \\\\ \\left\\langle 1,t^n \\right\\rangle  & ... &  &\\left\\langle t^n,t^n \\right\\rangle \\end{bmatrix}w^{\\star}\\] and thus, \\[ \\begin{bmatrix} \\left\\langle f,1 \\right\\rangle \\\\ \\left\\langle f,t\\right\\rangle\\\\ \\left\\langle f,t \\right\\rangle \\\\ ... \\\\ \\left\\langle f,t^n \\right\\rangle\\end{bmatrix} = \\begin{bmatrix} \\left\\langle 1,1 \\right\\rangle & \\left\\langle t,1 \\right\\rangle & ... & \\left\\langle t^n,1 \\right\\rangle \\\\ \\left\\langle 1,t\\right\\rangle\\\\ \\left\\langle 1,t^2 \\right\\rangle \\\\ ... \\\\ \\left\\langle 1,t^n \\right\\rangle  & ... &  &\\left\\langle t^n,t^n \\right\\rangle \\end{bmatrix}w^{\\star}\\]\nFinally, \\[ w^{\\star} = \\begin{bmatrix} \\left\\langle 1,1 \\right\\rangle & \\left\\langle t,1 \\right\\rangle & ... & \\left\\langle t^n,1 \\right\\rangle \\\\ \\left\\langle 1,t\\right\\rangle\\\\ \\left\\langle 1,t^2 \\right\\rangle \\\\ ... \\\\ \\left\\langle 1,t^n \\right\\rangle  & ... &  &\\left\\langle t^n,t^n \\right\\rangle \\end{bmatrix}^{-1} \\begin{bmatrix} \\left\\langle f,1 \\right\\rangle \\\\ \\left\\langle f,t\\right\\rangle\\\\ \\left\\langle f,t \\right\\rangle \\\\ ... \\\\ \\left\\langle f,t^n \\right\\rangle\\end{bmatrix}\\] In general the matrix \\(G_{ij} = \\langle b_i,b_j\\rangle\\) is called the Gram matrix and it depends only on the basis functions \\(b_i\\). Thus in general, the “best approximation” of \\(f\\) in the basis \\(b_i\\) is given by \\[ w^{\\star} = G^{-1}\\begin{bmatrix} \\langle f,b_0\\rangle \\\\ \\langle f,b_1\\rangle \\\\ \\langle f,b_2\\rangle \\\\ ... \\\\ \\langle f,b_n\\rangle \\\\ \\end{bmatrix}\\]"
  },
  {
    "objectID": "tech.html",
    "href": "tech.html",
    "title": "Technical Notes",
    "section": "",
    "text": "Projections and Basis Functions with Chemistry\n\n\n\n\n\n\nObservable Functions\n\n\nProjection Operators\n\n\n\n\n\n\n\n\n\nFeb 19, 2025\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nTAI Note\n\n\n\n\n\n\nTAI\n\n\n\n\n\n\n\n\n\nJul 16, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nObservable functions and their dynamics\n\n\n\n\n\n\nObservable Functions\n\n\nProjection Operators\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction and Motivations for a Projection Operator Approach\n\n\n\n\n\n\nProjection Operators\n\n\nDynamical Closure\n\n\n\n\n\n\n\n\n\nJun 18, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nProjection in terms of the Gram Matrix\n\n\n\n\n\n\nGram Matrix\n\n\nProjection Operators\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nAn interesting connection between sloppy model analysis and projection operators\n\n\n\n\n\n\nSloppy Models\n\n\nProjection Operators\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Living Physics Lab Notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjections and Basis Functions with Chemistry\n\n\n\n\n\n\nObservable Functions\n\n\nProjection Operators\n\n\n\n\n\n\n\n\n\nFeb 19, 2025\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Built This Quarto Notes Site\n\n\n\n\n\n\nBuilds\n\n\nNotes\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Generation Bioreactor Build Note\n\n\n\n\n\n\nBuilds\n\n\nBioreactor\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nDavid Jordan, Michele Cespa\n\n\n\n\n\n\n\n\n\n\n\n\nSailing Away\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\nSep 14, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To Concepts\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\nAug 14, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nTAI Note\n\n\n\n\n\n\nTAI\n\n\n\n\n\n\n\n\n\nJul 16, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nObservable functions and their dynamics\n\n\n\n\n\n\nObservable Functions\n\n\nProjection Operators\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction and Motivations for a Projection Operator Approach\n\n\n\n\n\n\nProjection Operators\n\n\nDynamical Closure\n\n\n\n\n\n\n\n\n\nJun 18, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nProjection in terms of the Gram Matrix\n\n\n\n\n\n\nGram Matrix\n\n\nProjection Operators\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nAn interesting connection between sloppy model analysis and projection operators\n\n\n\n\n\n\nSloppy Models\n\n\nProjection Operators\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "concept_posts/welcome/index.html",
    "href": "concept_posts/welcome/index.html",
    "title": "Welcome To Concepts",
    "section": "",
    "text": "This is the first build post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "build_posts/quarto_blog_build/index.html",
    "href": "build_posts/quarto_blog_build/index.html",
    "title": "How I Built This Quarto Notes Site",
    "section": "",
    "text": "This note describes how I set up and hosted this quarto notes site on the notes.x subdomain of my domain livingphysics.org. To do this I run quarto locally in Visual Studio Code and serve the site using a DigitalOcean app with a custom domain that is managed by SquareSpace. As of this writing (September ’24) Digital Ocean allows you to create 3 free apps.\nQuarto is a\nbased on markdown that allows for equations and code to be easily embedded into posts. This guide provides an excellent overview of setting up a quarto notes site and how to author individual posts as well as the basics of markdown. I referred to this to set up the website, the different notes sections, the main index, and the RSS feed. This site is separately hosted from the my main website, which I wrote mostly in html and is hosted on github pages with a custom domain."
  },
  {
    "objectID": "build_posts/quarto_blog_build/index.html#guide",
    "href": "build_posts/quarto_blog_build/index.html#guide",
    "title": "How I Built This Quarto Notes Site",
    "section": "Guide",
    "text": "Guide\n\nSetting up my local environment\n\nFirst I installed Visual Studio Code and added the Quarto, Julia, Code Spell Checker, and the Scientific Terms extensions via the extensions sidebar in the left panel. The code spell checker extension allows for mistake highlighting in VS code, and the shortcut for making a correction is to click on the underlined word and press Command + Period(.) on my Mac.\nI render the website locally using the quarto render command and the files are built into the docs subdirectory as the following code is in the _quarto.yml file in the quarto_notes directory.\n\nproject:\n  type: website\n  output-dir: docs\n\nThe quarto_notes directory is a git repository that is synced to a public github repository. I created this using the github desktop client. This is convenient because after rendering, it is simple to git commit and git push the changes to your public repository. Downstream, the app will be configured to rebuild you site after a new commit is pushed. The repository can be found here. This has a few benefits:\n\n\nas it is a public repository, others can view the source code for the site directly which can help them replicate parts of the site\nthere is robust version control and version history, which allows changes to posts to be tracked over time. This provides a record of revisions\ncomments and issues (both technological and scientific) can be opened using the GitHub issues feature.\n\nEventually, I would like to integrate a commenting system as well, but as this site is currently served as a static site, it likely requires an upgrade to a paid Digital Ocean app.\n\n\nSetting up the remote environment.\nI referred to this guide provided by Digital Ocean no how to set up a static site app. The only difference is that you will need to specify the source directory as docs. Make sure auto deploy is on. You can find this in settings by clicking on the component called your_repository-docs and can change it in the sections called Source. The guide was simple and worked flawlessly so I will only describe below how I got my app to point to my custom subdomain. Digital ocean also provides a very good guide for this.\n\nYou will need to purchase or otherwise obtain a domain name. My domains are managed by SquareSpace domains, and were inherited here from Google domains. If you are choosing a domain provider and want to use the Obsidian publish feature with your domain as well, I suggest using Cloudfare.\nOn the Digital Ocean app dashboard for your site, in the top left there is a button that says Actions. If you click this one action is manage domains. You can also get here by clicking the setting tab and scrolling down to Domains. Here you will find the IP address for your app, mine looks like app_ip_address.ondigitalocean.app. You will need this for the CNAME record in the next step. Here you will also see a button called Add Domain. Click it and add the subdomain you want to point to your app. Mine is notes.livingphysics.org.\nOn the SquareSpace domain management site, there is a sidebar option called DNS and a sub option called DNS Settings. Here you can add custom records. I added a CNAME record as shown below. Digital Ocean provides a guide for this in general here.\n\nHost    Type    Priority    Data\nnotes   CNAME   ---         app_ip_address.ondigitalocean.app\nThat should be everything you need to get up and running with your site. As always with these things, there is probably a ton of latent knowledge I have neglected to share, so don’t hesitate to reach out with questions. I will also periodically provide updates to this post to address points that are unclear or poorly explained."
  },
  {
    "objectID": "build_posts/gen1_reactor_build/index.html",
    "href": "build_posts/gen1_reactor_build/index.html",
    "title": "First Generation Bioreactor Build Note",
    "section": "",
    "text": "This note describes the design and construction of a RaspberryPi based 4-bioreactor system that has external illumination, external temperature recording, and magnetic stirring. The system monitors internal temperature and pressure using the BME280 series sensor and monitors turbidity via Infrared absorbance (\\(180^{\\circ}\\) ) and scattering (\\(135^{\\circ}\\)). This system is based on the sealed long-term ecosystems designed by the Kuehn Lab [1].\n\n\n\nFirst Generation Bioreactor (x4)"
  },
  {
    "objectID": "build_posts/gen1_reactor_build/index.html#stirring",
    "href": "build_posts/gen1_reactor_build/index.html#stirring",
    "title": "First Generation Bioreactor Build Note",
    "section": "Magnetic Stirring",
    "text": "Magnetic Stirring\nMagnetic stirring is done using PWM controlled PC fans as with magnetics attached. For this version, I have used Noctua NF-A6x25 Fans, which are 12V and 60mm square. The have anti-vibration pads and can be mounted directly to the 15mm construction rail we have used as a frame. The frame consists of 2x 270mm and 4x150mm beams (more details in Construction and Assembly). A pair of magnets is attached to the free face of each fan (the underside) with one “face up” next to one “face down” using 3M double stick foam tape. The fans include a Y-adapter which can be used to make a tree so that all fans can be powered and controlled with a single connector. The magnets spin a 10mm stir bar placed inside each vial."
  },
  {
    "objectID": "build_posts/gen1_reactor_build/index.html#illumination",
    "href": "build_posts/gen1_reactor_build/index.html#illumination",
    "title": "First Generation Bioreactor Build Note",
    "section": "Programmable Illumination",
    "text": "Programmable Illumination\nIllumination is provided from below using a Neopixel 8 LED ring, mounted above the fan. This allows for arbitrary RGB coloring. The NeoPixel library allows the specification of color in 3-element RGB format. The rings are connected in series, but fully addressable individually. Sample code for programming the rings is provided in the git repository in the build directory. A custom base which allows for the cables to be daisy chained as well as a holding plate for each ring were laser cut. The assembly is outlined below."
  },
  {
    "objectID": "build_posts/gen1_reactor_build/index.html#turbidity",
    "href": "build_posts/gen1_reactor_build/index.html#turbidity",
    "title": "First Generation Bioreactor Build Note",
    "section": "Turbidity Measurement Layer",
    "text": "Turbidity Measurement Layer\nTurbidity is measured using simple photodiode circuits to convert photons into a voltage that is read by one of the two analog to digital converters (ADCs) and read over the i2c protocol on the Raspberry Pi. We have employed a 4-channel 16 bit ADC and an 8-channel 8 bit ADC to record the 12 photodiode signals. 4 pass-thru absorbance signals at (\\(180^{\\circ}\\) ) and 4 scattering signals at (\\(135^{\\circ}\\)) and 4 IR LED output reference signals. This design was adapted from a similar design in the commerically available PioReactor. We have 4 of these that will be used for another part of the project. Each of the LEDs and photodiodes is held in the correct orientation and location by placing it in an appropriately shaped laser cut-out. The LEDs were provided a constant current that could be switched on and off using a FemtoBuck."
  },
  {
    "objectID": "build_posts/gen1_reactor_build/index.html#surface_temp",
    "href": "build_posts/gen1_reactor_build/index.html#surface_temp",
    "title": "First Generation Bioreactor Build Note",
    "section": "Vial Surface Temperature Layer",
    "text": "Vial Surface Temperature Layer\nAbove the Turbidity sensor layer, there are 4 DS18B20 one-wire temperature probes that monitor the temperature at the outside surface of the vial. Using the 1-wire protocol allows fro all of these to be wired together in parallel, which is greatly simplified by using these daisy chain wires1 from Mouser."
  },
  {
    "objectID": "build_posts/gen1_reactor_build/index.html#pressure",
    "href": "build_posts/gen1_reactor_build/index.html#pressure",
    "title": "First Generation Bioreactor Build Note",
    "section": "Internal Pressure and Temperature Sensors",
    "text": "Internal Pressure and Temperature Sensors\nThe internal pressure sensors are mounted in the lids of the vials in a manner similar to the one described in  [1]. First, 4 holes were laser cut into each lid to allow a 4-pin male-male header to pass through snugly. This was then sealed with hermetic sealing epoxy (Epo-tek H74). These reactors are designed for experiments much shorter than those described in  [1], so less expensive epoxy could probably be used. Each BME280 sensor board was soldered with a 4-pin female header. On the outside, the same daisy chain wires were used to provide power to each sensor board in parallel, and an individual wires were attached to the pins for each SDA and SCL pin and connected to a 4-channel multiplexer."
  },
  {
    "objectID": "build_posts/gen1_reactor_build/index.html#external_temp",
    "href": "build_posts/gen1_reactor_build/index.html#external_temp",
    "title": "First Generation Bioreactor Build Note",
    "section": "External Temperature",
    "text": "External Temperature\nFinally an external temperature sensor (PCT2075) was added in series with all fo the other i2c components to monitor the fluctuations in temperature in the room, due mostly to changes in the building-wide heating and cooling system. As this design does not incorporate onboard temperature control, the entire rector can be either placed in an incubator or in a temperature controlled room if desired. The next generation reactor will incorporate onboard temperature control."
  },
  {
    "objectID": "build_posts/gen1_reactor_build/index.html#assembly",
    "href": "build_posts/gen1_reactor_build/index.html#assembly",
    "title": "First Generation Bioreactor Build Note",
    "section": "Construction and Assembly",
    "text": "Construction and Assembly\n\nGeneral Notes on Electronics\nAll of the sensor boards are powered by a single power supply and a combination of LM7805 and LM7812 +5V and +12V voltage regulators. The i2c sensor boards have QWIIC connectors which pass through the power and the SDA/SCL signals. Because these have unique i2c addresses they can all be read independently (the four BMEs have the same i2c addresses and require the multiplexer, in general the same devices have the same address, but this can be altered to some extent with address jumper pins. Multiplexing was more convenient in this case). A variety of QWIIC connectors can be found here\n\n\nPhysical Construction\nThe final construction is diagramed below. The vials are held in a layered structure built up from custom cut pieces of 5mm acrylic sheet. The diagrams for the different layers are in the cad_files directory of the project git.\n\n\n\nConstruction Diagram: In this diagram, the light grey rectangles represent 2mm clear acrylic and the black rectangles are 5mm black acrylic. Red and Blue holes are holes for M3 screws to pass through The double hole plates have holes large enough to accommodate the screw heads.\n\n\nThe basic build connects two H-frames made from a 270mm construction rail with the fans themselves. The 4 fans comprise 240mm leaving 10mm between the fans. The vials themselves are spaced 70mm center to center. The idea is that each sensor layer is comprised of three acrylic sheet layers, a bottom and otp layer, and then a middle layer which has the cutouts for the individual components as shown in the .dxf files. Attachments to the construction rails are done with M3 screws and hex nuts. 3mm M3 screws for constructing the H-frame and 6mm screws for attaching the fans to the rails. These can be drop in T-nuts but if you are using hex nuts the fans will have to be slid into place one at a time."
  },
  {
    "objectID": "build_posts/gen1_reactor_build/index.html#footnotes",
    "href": "build_posts/gen1_reactor_build/index.html#footnotes",
    "title": "First Generation Bioreactor Build Note",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(194:Red, 195:Black)↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Overview\nThis is a repository for notes to share some aspects of the problems we are working on. Technical notes are generally theoretical, involve calculations, and aim to illustrate something formal. Concept notes are more informal, and aim to show how I am thinking about problems. Build notes are intended to aid in understanding and replicating our engineering designs."
  },
  {
    "objectID": "builds.html",
    "href": "builds.html",
    "title": "Build Notes",
    "section": "",
    "text": "How I Built This Quarto Notes Site\n\n\n\n\n\n\nBuilds\n\n\nNotes\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Generation Bioreactor Build Note\n\n\n\n\n\n\nBuilds\n\n\nBioreactor\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nDavid Jordan, Michele Cespa\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "concepts.html",
    "href": "concepts.html",
    "title": "Concept Notes",
    "section": "",
    "text": "Welcome To Concepts\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\nAug 14, 2024\n\n\nDavid Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/sailing/index.html",
    "href": "notes/sailing/index.html",
    "title": "Sailing Away",
    "section": "",
    "text": "Recently I have started an independent research lab outside of (but adjacent to) academia. Upon hearing this, many people have commented: “that sounds like a lot of work”. It is and it isn’t, to steal a great analogy from Adam Mastroianni, setting out on a little sailboat of your own is a lot different than having a cabin on a (the) big ship. You need a lot of different skills, navigation, basic meteorology, how to tie knots, knowing what a jib is, how to fix the electronics, how to swim, etc. Comparatively, on the big ship, you need to learn how to get your food from the mess, how to get items delivered through the supply dock, how to convince the captain to take a little detour to see those cool birds over there, and how to impress the funders of the big ship when they come aboard to see what you’ve been doing. The question of whether it is more or less work seems like the wrong question, so the wrong answer is “I feel like it’s more or less the same, possibly a bit more” but the right answer is, it’s completely different work, that at the end of the day makes me energized rather than drained. The connection between the work and the freedom is direct and that is invigorating. I was worried when I started out that “sailing the boat” would take too much time away from doing the actual research. When I started my goal was to spend 95% of my time doing the research, in the first months, I can’t claim to have met that, maybe I spend 90% of my time, but I am also spending more time overall and am enjoying it immensely. It’s true that a little boat can’t go everywhere. The big boat has engines and GPS, it can go almost anywhere it wants to (but not anywhere you want to). Your sailboat is subject to the winds and seas, and sometimes these can’t take you where you want to go. If you want to go roughly where the big ship is headed the big ship can be a great choice! But if your the type that feels like taking out a sailboat sounds a lot better than getting a cabin on a cruise ship, but you don’t know anything about reading the wind or navigating by the stars, reach and I’d be more than happy to “show you the ropes”."
  },
  {
    "objectID": "tech_posts/TAI/TAI.html",
    "href": "tech_posts/TAI/TAI.html",
    "title": "TAI Note",
    "section": "",
    "text": "This note concerns the use of data transformations in calculations of the Transcriptome Age Index or TAI. Here I argue that the TAI, when calculated as an expression weighted sum of phylostratigraphic indices is a vector projection and thus the length of the relative expression vector should be normalized to account for the fact that expression vectors are shorter when more genes are expressed. Mathematically, the square root transform achieves this normalization because of a property of histograms (transforming an n-simplex into an n-sphere)."
  },
  {
    "objectID": "tech_posts/TAI/TAI.html#the-square-root-transform",
    "href": "tech_posts/TAI/TAI.html#the-square-root-transform",
    "title": "TAI Note",
    "section": "The Square Root Transform",
    "text": "The Square Root Transform\nLet’s begin with the definition of the \\(TAI\\) at stage \\(s\\): \\[TAI(s) = \\sum_i^N p_i \\frac{e_i(s)}{\\sum_i^Ne_i(s)}\\] where, \\(e_i(s)\\) is the expression of gene \\(i\\) in stage \\(s\\) and \\(p_i\\) is the measure of gene age (phylostratum) of gene \\(i\\) and where there are \\(N\\) genes total.\nIf we define the normalized expression \\[n_i(s) = \\frac{e_i(s)}{\\sum_i^Ne_i(s)}\\] This can be rewritten as \\[TAI(s) =  \\langle p_i,n_i(s)\\rangle\\]where \\(\\langle\\cdot,\\cdot\\rangle\\) is the inner (dot) product. Thus, we are taking the dot product of 2 vectors, the phylostratum vector and the normalized expression vector. By construction, the normalized expression vector satisfies the property \\(\\sum_i^N n_i(s)=1\\). The space of possible normalized expression vectors is then the N-simplex.\n\n\nCode\nusing Plots\n\nvertices = [\n    [1.0, 0.0, 0.0],  # Unit vector in x direction\n    [0.0, 1.0, 0.0],  # Unit vector in y direction\n]\n\n# Create the plot\np = plot(\n    xlabel=\"X\", ylabel=\"Y\",\n    title=\"Standard 1-Simplex\",\n    legend=false,\n    aspect_ratio=:equal,\n)\n\n# Plot all edges of the tetrahedron\nfor i in 1:2\n    for j in (i+1):2\n        v1, v2 = vertices[i], vertices[j]\n        plot!(p, [v1[1], v2[1]], [v1[2], v2[2]], \n              color=:blue, linewidth=2)\n    end\nend\n\n# Plot vertices as points\nscatter!(p, [v[1] for v in vertices], [v[2] for v in vertices], \n         color=:blue, markersize=5)\n\n# Set the axis limits\nplot!(p, xlim=(-0.1, 2.5), ylim=(-0.1, 2.5))\n\n# Display the plot\nvector = [1, 2]\nquiver!(p, [0], [0], quiver=([vector[1]], [vector[2]]), \n        color=:purple, linewidth=2, arrow=arrow(:closed, 0.1))\n\nexp_vec = [\n    [1, 0],\n    [0, 1],\n    [0.5, 0.5]\n]\n\nfor v in exp_vec\n    quiver!(p, [0], [0], quiver=([v[1]], [v[2]]), color=:red, linewidth=2, arrow=arrow(:closed, 0.1))\nend\n\ndisplay(p)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: 2-Gene Simplex Plot\n\n\n\n\nThis diagram depicts an example with 2 genes, in this case the possible normalized expression vectors are shown as the blue simplex, with 3 example expressions shown as the red vectors. A hypothetical phylostratum vector with gene age 1 for gene X and age 2 for gene Y is shown in purple, but this analysis does not depend on the particular form of \\(p_i\\). What happens when we calculate the TAI for different possible normalized expression vectors along the simplex (blue)? In general, the dot product can be calculated as \\[\\langle p_i,n_i(s) \\rangle = \\left\\lVert p_i\\right\\rVert \\left\\lVert n_i(s)\\right\\rVert cos(\\theta)\\]\nWhen comparing the TAI between different stages, the phylostratum vector is fixed, so \\(\\left\\lVert p_i\\right\\rVert\\) is constant. As expression patterns change between stages however, we would like to see how these changes affect the projection of \\(n_i(s)\\) onto \\(p_i\\). This projection has 2 components, \\(\\left\\lVert n_i(s)\\right\\rVert\\) and \\(cos(\\theta)\\). However, the magnitude of \\(n_i(s)\\) is not constant, in fact near the vertices, the magnitude is larger than near the center of the simplex (equal expression of all genes). This implies that in this formulation, stages which have fewer genes expressed or a small number of more highly expressed genes (and thus normalized expression vectors nearer to the vertices) will have a necessarily larger TAIs regardless of which genes are expressed. This is not a feature we would like to have in the TAI, and in fact this feature gets much worse the more genes we have. In the 2 gene example, the magnitude of \\(n_i\\) at the center is \\(0.7071\\) times the magnitudes at the vertices. As the number of genes goes up, this factor decreases even more.\n\n\nCode\nusing LinearAlgebra  # For the norm() function\nusing Plots  # For plotting\n\n# Initialize an array to store the norms\nl = zeros(999)\n\n# Main loop\nfor i in 2:1000\n    x = ones(i)  # In Julia, this creates a vector, not a matrix\n    x = x / sum(x)\n    l[i-1] = norm(x)\nend\n\n# Plot the results\nplot(l, xlabel=\"Number of Genes\", ylabel=\"Magnitude of Center\", title=\"Norm of Centered Expression Vector\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Magnitude of the center vector\n\n\n\n\nThe simple solution is to transform the expression vectors so that they all have unit length. This is easy enough to do, but because these values fall on a simplex, taking the square root of the normalized expression vector is a convenient way to carry out this transformation. This works because \\(\\sum_i^N n_i(s)=1\\), thus if we take the square root at this stage, and let \\(r_i(s)= \\sqrt{n_i(s)}\\).\n\\[\\sum_i^N r_i(s)^2=1\\] and thus the \\[\\left\\lVert r(s)\\right\\rVert = \\sqrt{\\sum_i^N r_i(s)^2} = 1\\]\nNow the set of possible transformed expression vectors is the unit N-sphere rather than the simplex.\n\n\nCode\nusing Plots\n\nvertices = [\n    [1.0, 0.0, 0.0],  # Unit vector in x direction\n    [0.0, 1.0, 0.0],  # Unit vector in y direction\n]\n\n# Create the plot\np = plot(\n    xlabel=\"X\", ylabel=\"Y\",\n    title=\"Standard 1-Sphere\",\n    legend=false,\n    aspect_ratio=:equal,\n)\n\n# Plot n-sphere\nfunction quarter_circle(t)\n    x = cos.(t)\n    y = sin.(t)\n    return x, y\nend\n\n# Generate points\nt = range(0, π/2, length=100)\nx, y = quarter_circle(t)\n\n# Create the plot\nplot!(x, y, \n    aspect_ratio=:equal, \n    label=\"Quarter Circle\",\n    linewidth=2,\n    color=:blue\n)\n\n# Plot vertices as points\nscatter!(p, [v[1] for v in vertices], [v[2] for v in vertices], \n         color=:blue, markersize=5)\n\n# Set the axis limits\nplot!(p, xlim=(-0.1, 2.5), ylim=(-0.1, 2.5))\n\n# Display the plot\nvector = [1, 2]\nquiver!(p, [0], [0], quiver=([vector[1]], [vector[2]]), \n        color=:purple, linewidth=2, arrow=arrow(:closed, 0.1))\n\nexp_vec = [\n    [1, 0],\n    [0, 1],\n    [0.7071, 0.7071]\n]\n\nfor v in exp_vec\n    quiver!(p, [0], [0], quiver=([v[1]], [v[2]]), color=:red, linewidth=2, arrow=arrow(:closed, 0.1))\nend\n\ndisplay(p)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: 2-Gene Sphere Plot\n\n\n\n\nThe issue of the form of \\(p_i\\), for example whether it should be a quantile rank, is separate from this issue."
  },
  {
    "objectID": "tech_posts/observable__dynamics/observable__dynamics.html",
    "href": "tech_posts/observable__dynamics/observable__dynamics.html",
    "title": "Observable functions and their dynamics",
    "section": "",
    "text": "In the previous note I concluded with these points about observable functions and projection,\nAt this point I hope you are comfortable with the concentration of dimension via function projection, in this note, I want to explore the expansion of dimension by observable functions. The main reason we would expand dimensionality is that it can make some computations easier, and that it can linearize a non-linear dynamical system. Consider the following example1\n\\[ \\Large \\begin{align} \\dot{x_1} &= \\mu x_2 \\\\ \\dot{x_2} &= \\lambda(x_2-x_1^2) \\end{align}\n\\] This is a 2D non-linear dynamical system because of the \\(x_1^2\\) term. Lets propose a set of observable functions \\(y_i\\) with the goal of linearizing the system.\n\\[ \\begin{bmatrix} y_1\\\\y_2\\\\y_3\\end{bmatrix} = \\begin{bmatrix} x_1\\\\x_2\\\\x_1^2\\end{bmatrix} \\] Now lets look at the dynamics of \\(y\\)\n\\[ \\Large \\begin{align} \\dot{y_1} &= \\mu x_2 = \\mu y_2 \\\\ \\dot{y_2} &= \\lambda(x_2-x_1^2) = \\lambda(y_2-y_3)\\\\ \\dot{y_3} &= 2x_1\\dot{x_1}\\\\ &= 2y_1(\\mu y_1) \\\\ &=2\\mu y_3 \\end{align}\n\\]\ngiving, \\[\\large \\frac{d}{dt}\\begin{bmatrix} y_1\\\\y_2\\\\y_3\\end{bmatrix}=\\begin{bmatrix} 0&\\mu&0 \\\\ 0 & \\lambda & -\\lambda \\\\ 0 & 0 & 2\\mu\\end{bmatrix} \\begin{bmatrix} y_1\\\\y_2\\\\y_3\\end{bmatrix}\\] The observable functions for this particular system also have the closure property I mentioned in the last note. We have expanded the dimensionality of the system, but the new system of observable functions has dynamics which are only functions the the \\(y_i\\) themselves. This need not be the case. For example, lets look at the example of the following 1D nonlinear dynamics. \\[\\Large \\dot{x_1} = -\\nu x_1^2 + (\\lambda-\\nu)x_1 + \\lambda\\] If we try the same trick here, \\[ \\begin{bmatrix} y_1\\\\y_2\\end{bmatrix} = \\begin{bmatrix} x_1\\\\x_1^2\\end{bmatrix} \\]\nWe have \\[ \\] \\[\\Large \\begin{align} \\dot{y_1} &= \\dot{x_1} = -\\nu y_2 + (\\lambda-\\nu)y_1 + \\lambda \\\\ \\dot{y_2} &= 2x_1\\dot{x_1} = 2y_1(-\\nu y_2 + (\\lambda-\\nu)y_1 + \\lambda)\\\\ &= -2\\nu y_1y_2+2(\\lambda-\\nu)y_y+2\\lambda y_1 \\\\ \\end{align}\\] Unfortunately here we see a problem, \\(y_1y_2\\) is not a linear function of our set of observables. \\(y_1y_2 = x_1^3\\), so we would need to add this to our observable functions, \\[ \\begin{bmatrix} y_1\\\\y_2\\\\y_3\\end{bmatrix} = \\begin{bmatrix} x_1\\\\x_1^2\\\\x_1^3\\end{bmatrix} \\] with \\[ \\Large \\begin{align} \\dot{y_3} &= 3x_1^2\\dot{x_1} = 3y_2(-\\nu y_2 + (\\lambda-\\nu)y_1 + \\lambda) \\\\ &= -3\\nu y_2^2+3(\\lambda-\\nu)y_1y_2 +3\\lambda y_2 \\\\ &= -3\\nu y_2^2+3(\\lambda-\\nu)y_3 +3\\lambda y_2 \\end{align}\\] Again we have a term that is not in our current observable set \\(y_2^2 = x^4\\). We can however continue this process for an infinite number of terms, if we introduce \\(y_0\\) to account for the constant term, we can write these dynamics with a matrix of the form:\n\\[\\Large \\frac{d\\vec{y}}{dt} = \\begin{bmatrix} 0&0&0&0&0&...&0\\\\ \\lambda & \\lambda-\\nu & -\\nu & 0& 0&... & 0 \\\\ 0 & 2\\lambda & 2(\\lambda-\\nu) & -2\\nu & 0 &...&0 \\\\ 0 & 0& 3\\lambda & 3(\\lambda-\\nu) & -3\\nu &...&0 \\\\ ...&&&&&&...\\end{bmatrix} \\vec{y}\\] So we have in infinite dimensional linear operator which has the same dynamics as our non-linear system. In fact, we are guaranteed to always be able find such an infinite dimensional linear operator2\nWhy is this useful though? We have traded non-linear dynamics for infinite dimensional linear dynamics. Other than control theory applications, its not clear why linear dynamics are important, and also, any real control system is finite dimensional. At this point the connection to orthogonality is hopefully emerging. The Laplacian dynamics from our master equation formulation is a linear operator. It is finite dimensional, but we saw in the last note how we can concentration dimension with projection operators. We can do this as well with the infinite dimensional operators like the one above. The Galerkin projection, that is the one that simply ignores most of the dimensions, is easiest, but like the example in the previous note, if the subspace onto which you are projecting is not closed, the dynamics floats off over time. In other words, the projection does not exactly recapitulate the dynamics, but rather is an approximation. Can chemical reaction networks use their Laplacian dynamics as such an approximation? The questions that I will try to answer in the next few notes are\nAs a teaser, lets go back to the one dimensional dynamical system that we needed an infinite expansion to represent.\n\\[\\Large \\dot{x_1} = -\\nu x_1^2 + (\\lambda-\\nu)x_1 + \\lambda\\] With our naive choice of observable functions, we saw that we needed an infinite dimensional dynamical linear dynamics to represent it in that set of observables. In fact any finite Galerkin projection in that basis is a pretty bad approximation, but what if we are more clever with the basis functions we select. The form of this equation is written in this way for a reason. Lets go back to the simple chemical reaction network from the previous note \\[\n\\Large\n\\ce{a&lt;=&gt;[{\\lambda}][{\\nu}]b}\n\\] with its linear dynamics \\[\\frac{d}{dt}\\begin{bmatrix}a\\\\b\\end{bmatrix} = \\begin{bmatrix}-\\lambda & \\nu \\\\ \\lambda & -\\nu\\end{bmatrix}\\begin{bmatrix}a\\\\b\\end{bmatrix}\\] If I choose an observable function of this dynamics to be the ratio of the two species \\[\\Large x_1 = \\frac{b}{a}\\] then the dynamics of \\(x_1\\) are given by \\[\\Large \\begin{align} \\dot{x_1} &= \\frac{\\dot{b}a-\\dot{a}b}{a^2} \\\\ &= \\frac{(\\lambda a-\\nu b)a - (-\\lambda a + \\nu b)b}{a^2} \\\\ &= \\frac{\\lambda a^2 - \\nu a b +\\lambda a b -\\nu b^2}{a^2} \\\\ &= -\\nu\\frac{b^2}{a^2}+(\\lambda-\\nu)\\frac{ab}{a^2}+\\lambda\\frac{a^2}{a^2}\\\\&= -\\nu x_1^2+(\\lambda-\\nu)x_1+\\lambda \\end{align}\\] So it turns out that the dynamics of this non-linear system can in fact be recapitulated by a closed linear system, generated by a Laplacian chemical reaction network. If our observable function is the ratio of two species.\nAt this point I hope you feel comfortable. 1) Reducing dimensionality with projection operators. Sometimes this projection can be exactly preserving for closed, invariant subspaces. Otherwise its an approximation 2) Expanding dimensionality through the use of observable functions. Sometimes we need to expand infinitely to get linear dynamics. But linear dynamics may not be the appropriate end goal. Ultimately I’m interested in observable dynamics that can be recapitulated with observables realizable by chemical reaction networks. These may be linear, Linearity is nice for our Laplacian dynamics, but even these are approximations based on time scale separation3 3) Although I didn’t explain it in detail in the last note (I will in a companion note) The transformation I used to find a closed projection was based on the eigen-decomposition of the linear dynamics given by \\(\\frac{d}{dt}\\begin{bmatrix}a\\\\b\\end{bmatrix} = \\begin{bmatrix}-\\lambda & \\nu \\\\ \\lambda & -\\nu\\end{bmatrix}\\begin{bmatrix}a\\\\b\\end{bmatrix}\\). This is another nice property of linearized dynamics, we can find eigenfunctions which have characteristic time scales and which are uncoupled (if this is unclear stay tuned for the next note). These eigen-decompositions as dimension preserving rotations of the dynamics.\nSo now we can take dynamical systems, expand their dimensionality, contract their dimensionality, and rotate their representations at will. The math here is not new or that difficult, but the new part (I hope) will be in seeing how chemical reaction networks can do this using only local information in a self organized way."
  },
  {
    "objectID": "tech_posts/observable__dynamics/observable__dynamics.html#footnotes",
    "href": "tech_posts/observable__dynamics/observable__dynamics.html#footnotes",
    "title": "Observable functions and their dynamics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBrunton SL, Brunton BW, Proctor JL & Kutz JN (2016) Koopman Invariant Subspaces and Finite Linear Representations of Nonlinear Dynamical Systems for Control. PLoS One 11: e0150171↩︎\nKoopman BO (1931) Hamiltonian Systems and Transformation in Hilbert Space. Proc Natl Acad Sci U S A 17: 315–318↩︎\nMirzaev I & Gunawardena J (2013) Laplacian dynamics on general graphs. Bull Math Biol 75: 2118–2149↩︎"
  },
  {
    "objectID": "tech_posts/proj_operator/proj_operator.html",
    "href": "tech_posts/proj_operator/proj_operator.html",
    "title": "Introduction and Motivations for a Projection Operator Approach",
    "section": "",
    "text": "This note introduces projection operators and their use in studying partially observed dynamical systems. The concept of dynamical closure is introduced, which is equivalent to dynamics living on a “Koopman invariant subspace”. The best introduction to this I feel is Chapter 8 of Zwanzig’s Nonequilibrium Statistical Mechanics. I will do my best to reintroduce the important points in summary below.\nThe basic premise of the projection operator approach begins with a complex, high dimensional dynamical system. In statistical mechanics this is often taken to be Hamilton’s equations of motion (so-called Hamiltonian Dynamics) for a large number of particles. The take home message of this section is that the projection operator framework provides a rigorous mathematical derivation, given some important and carefully chosen assumptions, of how to ignore almost all of the order \\(10^{23}\\) degrees of freedom in such a system and find dynamics of effective variables that satisfy some useful properties, the most useful of which is closure. Closure is the ideal that the dynamics of the effective variables can be written only in terms of the effective variables themselves. To make this a bit more concrete think of the concentration observable function. We know we can write the diffusion equation as: \\[\\frac{dC(x,t)}{dt} = D\\nabla^2C(x,t)\\]\nWhere the dynamics of the concentration is a function of only the concentration itself. Concentration is clearly coupled to the positional degrees of freedom of the full dynamical system (which are coupled to the velocity degrees of freedom), but we can ignore all of these with some well chosen assumptions (many of these are convenient assumptions about near equilibrium behavior or about time scale separations that allow appropriate averages to be taken analytically).\nReturning to projection operators, recall from my previous note about An interesting connection between “sloppy model analysis” and projection operators that function space projection is essentially what we are talking about here, and that Fourier analysis is still the clearest mental model I have of projection. In this context, if we have a large time scale separation between the fast and slow dynamics of a noisy oscillation, we can project the dynamics onto the slow modes (the signal) and treat the fast modes as noise. In general, I am not trying to estimate anything myself with projection operators, and I don’t know that biological systems are necessarily trying to estimate anything any more than diffusion is trying estimate anything in a purely physical process. I am more willing to say that\nIn general, despite evidence that projections may be the best way to understand system dynamics, the underlying systems are too heterogeneous in my opinion to offer an path forward toward rigorous treatments, The case of chemical reaction networks is the exception! In this system, I think if we restrict ourselves to mass action kinetics (a reasonable restriction based on everything we know about chemistry that nevertheless does not not limit expressivity) we can figure out both what the basis functions are and how the system projects onto them (inspired by our orthogonality work). To motivate this lets start with the simplest example, a two species chemical reaction.\n\\[\n\\Large\n\\ce{a&lt;=&gt;[{\\lambda}][{\\nu}]b}\n\\] We can write the Laplacian dynamics for this system as \\[\\frac{dx}{dt} = \\begin{bmatrix}-\\lambda & \\nu \\\\ \\lambda & -\\nu\\end{bmatrix}x\\] where \\(x=\\begin{bmatrix}a\\\\b\\end{bmatrix}\\).\nThis is a two dimensional dynamical system with couple degrees of freedom \\(a\\) and \\(b\\). Now if we wanted to reduce the dimensionality of the could do a Galerkin projection onto only one of the degrees of freedom and just simply ignore the other, for example for a we would have \\(\\dot{a}=-\\lambda a\\). This is dynamics of a only in terms of a, but these dynamics clearly diverge from the true dynamics significantly. This is probably not a very useful projection\nCode\nusing Plots\nusing LinearAlgebra\n\n# Define parameters\nλ = 0.5\nν = 1.5\nL = [-λ ν; λ -ν]\n\n# Create initial conditions\nn = 4\ntmp = range(0, 1, length=n)\nx0 = hcat([vcat(tmp[i], 1-tmp[i]) for i in 1:n]...)\n\n# Function to integrate linear system (you'll need to implement this)\nfunction integrate_linear(x0, L, dt, T)\n    t = 0:dt:T\n    x = zeros(length(x0), length(t))\n    x[:, 1] = x0\n    \n    for i in 2:length(t)\n        x[:, i] = x[:, i-1] + dt * (L * x[:, i-1])\n    end\n    \n    return x, t\nend\n\n# Create plot with dark theme\ntheme(:dark)\nplot(background_color=:black, \n     foreground_color=:white,\n     legend=:topright,\n     xlabel=\"time\",\n     ylabel=\"[a]\",\n     fontfamily=\"Computer Modern\",\n     grid=false)\n\n# Plot true dynamics\nfor i in 1:n\n    x, t = integrate_linear(x0[:, i], L, 0.01, 5)\n    plot!(t, x[1, :], linewidth=2, label=i == 1 ? \"True Dynamics\" : \"\",color=i)\nend\n\n# Plot projected dynamics\nfor i in 1:n\n    t = 0:0.1:5  # Using larger time steps for dashed lines\n    projected = x0[1, i] * exp.(-λ * t)\n    plot!(t, projected, linestyle=:dash, linewidth=2, \n          label=i == 1 ? \"Projected Dynamics\" : \"\", color=i)\nend\n\n# Calculate eigenvectors and transform initial conditions\nF = eigen(L)\nv = F.vectors\nz0 = inv(v) * x0\n\n# Style adjustments\nplot!(framestyle=:box,\n      fontsize=14,\n      margin=5Plots.mm)\nIs there a more useful one dimensional projection of this two dimensional system? What if we measured an observable function of \\(a\\) and \\(b\\)?. I will present such a function here and show how I determined it later. Assume for now that we first compute an observable of the system and then do a one dimensional projection onto this carefully chosen observable. The observable function is given as \\[\\Large c = \\frac{-\\lambda}{\\lambda+\\nu}a+\\frac{\\nu}{\\lambda+\\nu}b\\] Lets look at the dynamics of this observable function \\[\\Large \\begin{align} \\dot{c}  &= \\frac{-\\lambda}{\\lambda+\\nu}\\dot{a}+\\frac{\\nu}{\\lambda+\\nu}\\dot{b} \\\\ &= \\frac{-\\lambda}{\\lambda+\\nu}(-\\lambda a+\\nu b)+\\frac{ \\nu}{\\lambda+\\nu}(\\lambda a-\\nu b) \\\\ &= \\frac{ \\lambda^2a}{\\lambda+\\nu}+\\frac{- \\lambda \\nu b}{\\lambda+\\nu}+\\frac{ \\nu \\lambda a}{\\lambda+\\nu} -\\frac{ \\nu^2 b}{\\lambda+\\nu} \\\\ &= (-\\lambda-\\nu)\\left(\\frac{- \\lambda}{\\lambda+\\nu}a+\\frac{ \\nu}{\\lambda+\\nu}b\\right ) \\\\ &= (-\\lambda-\\nu)c\\end{align}\\]\nSo we have found an observable function that satisfies our closure property! that is, its dynamics, which are determined by the underlying two-dimensional dynamical system, are self-determined. We do not need knowledge of \\(a(t)\\) and \\(b(t)\\) explicitly to predict or model the dynamics of \\(c\\), only knowledge of \\(c\\) itself.\nCode\nusing Plots\nusing LinearAlgebra\n\n# Define parameters\nλ = 0.5\nν = 1.5\nL = [-λ ν; λ -ν]\n\n# Create initial conditions\nn = 4\ntmp = range(0, 1, length=n)\nx0 = hcat([vcat(tmp[i], 1-tmp[i]) for i in 1:n]...)\n\n# Function to integrate linear system (you'll need to implement this)\nfunction integrate_linear(x0, L, dt, T)\n    t = 0:dt:T\n    x = zeros(length(x0), length(t))\n    x[:, 1] = x0\n    \n    for i in 2:length(t)\n        x[:, i] = x[:, i-1] + dt * (L * x[:, i-1])\n    end\n    \n    return x, t\nend\n\n# Create plot with dark theme\ntheme(:dark)\nplot(background_color=:black, \n     foreground_color=:white,\n     legend=:topright,\n     xlabel=\"time\",\n     ylabel=\"[a]\",\n     fontfamily=\"Computer Modern\",\n     grid=false)\n# Calculate eigenvectors and transform initial conditions\nF = eigen(L)\nv = F.vectors\nz0 = inv(v) * x0\n\n# Plot true dynamics\nfor i in 1:n\n    x, t = integrate_linear(x0[:, i], L, 0.01, 5)\n    tmp_data = inv(v)*x;\n    plot!(t,tmp_data[1, :], linewidth=2, label=i == 1 ? \"True Dynamics\" : \"\",color=i)\nend\n\n# Plot projected dynamics\nfor i in 1:n\n    t = 0:0.5:5  # Using larger time steps for dashed lines\n    projected = hcat(exp.(-(λ + ν) * t), zeros(length(t)))\n    tmp_data = projected * z0[:, i]\n    scatter!(t, tmp_data, \n            marker=:square,\n            markersize=2,\n            linewidth=3,\n            label=i == 1 ? \"Projected Dynamics\" : \"\",\n            color=i)\nend\n\n\n# Style adjustments\nplot!(framestyle=:box,\n      fontsize=14,\n      margin=5Plots.mm)\nhis is a simplified example but the principle is general. Sometimes, carefully chosen observable functions allow you to reduce the dimensionality of a dynamical system. Ideally the dynamics on the lower dimensional observables will be closed, that is they stay on some subspace of the full Hilbert space.\nThe utility of projection may not be obvious when going form 2 to 1 dimensions, but when reducing from thousands of dimensions to a few the implications for controllability and robustness are clearer as the effective dynamics no longer explicitly depend on all the microscopic degrees of freedom.\nAt the end of this I hope that you have a very clear idea of three concepts which I will build on later when I make a connection between projection operator theory and our orthogonality work building up to finally describing the connection between orthogonality and the steady state to flux relationship ."
  },
  {
    "objectID": "tech_posts/proj_operator/proj_operator.html#footnotes",
    "href": "tech_posts/proj_operator/proj_operator.html#footnotes",
    "title": "Introduction and Motivations for a Projection Operator Approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the previous note I used the \\(L_2\\) inner product without an explicit measure, however in Stat mech the inner product is taken over the invariant measure. To know what this measure is often requires that systems are assumed to be near equilibrium.↩︎\nIdeally these closed sets of observable functions are linear and Markovian, In real cases ,they may be non-linear and non-Markovian. When they are approximately and not completely self-determined, we hope that we can make use of averages and time scale separations to treat the influence of the unobserved degrees of freedom as uncorrelated random noise↩︎"
  }
]